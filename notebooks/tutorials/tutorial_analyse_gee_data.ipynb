{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Analyse data downloaded from Google Earth Engine using pyveg \n",
    "\n",
    "\n",
    "Google Earth Engine is a powerful tool for obtaining and analysing satellite imagery. The pyveg package provides useful scripts for interacting with the Earth Engine API and downloading data. \n",
    "\n",
    "The location used in this tutorial is Tiger Bush vegetation from Niger in coordinates 2.59, 13.12. The downloaded data is a JSON file containing weather and network centrality metrics in a monthly basis from 2015 to 2020.\n",
    "\n",
    "Now let's use the functions provided by pyveg to run a simple analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "from pyveg.src.data_analysis_utils import *\n",
    "from pyveg.src.plotting import *\n",
    "from pyveg.src.image_utils import create_gif_from_images\n",
    "from pyveg.src.analysis_preprocessing import *\n",
    "from pyveg.scripts.analyse_gee_data import plot_feature_vector\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataset is a json file found in this directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results directory from `download_gee_data` script.\n",
    "json_summary_path =  'results_summary_TigerBush_Niger.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output figures will be saved in an `analysis` sub-directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put output plots in the results dir\n",
    "input_dir = '.'\n",
    "output_dir = os.path.join(input_dir, 'analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all json files in the directory and produce a dictionary of dataframes. Each key is a satellite, either weather related or image related (for the network centrality measures).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reading results from '{os.path.abspath(json_summary_path)}'...\")\n",
    "json_data = json.load(open(json_summary_path))\n",
    "\n",
    "ts_dirname, dfs = preprocess_data(\n",
    "        json_data, output_dir, n_smooth=4, resample=False, period=\"MS\"\n",
    "    )\n",
    "\n",
    "print (dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the output dataframe looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (dfs['COPERNICUS/S2'].head())\n",
    "print (dfs['ECMWF/ERA5/MONTHLY'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial analysis\n",
    "\n",
    "First, let's build 2D plots showing the network centrality values on the general 10km images for each date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new subdir for time series analysis\n",
    "spatial_subdir = os.path.join(output_dir, 'spatial')\n",
    "\n",
    "#if directory exists delete results from previous runs\n",
    "if os.path.exists(spatial_subdir):\n",
    "    shutil.rmtree(spatial_subdir)\n",
    "\n",
    "os.makedirs(spatial_subdir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial analysis and plotting \n",
    "# from the dataframe, produce network metric figure for each avalaible date\n",
    "print('\\nCreating spatial plots...')\n",
    "\n",
    "for collection_name, df in dfs.items():\n",
    "    if collection_name == 'COPERNICUS/S2' or 'LANDSAT' in collection_name:\n",
    "        # convert the dataframe of each image to geopandas and coarse its resolution slightly\n",
    "        data_df_geo = convert_to_geopandas(df.copy())\n",
    "        create_lat_long_metric_figures(data_df_geo, 'offset50', spatial_subdir)\n",
    "\n",
    "output_plots_name = create_gif_from_images(spatial_subdir,'output.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the result on a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "with open(output_plots_name,'rb') as f:\n",
    "    display(Image(data=f.read(), format='png',width=500, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average network centrality feature vectors ver all time points and sub images are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new subdir for time series analysis\n",
    "tsa_subdir = output_dir\n",
    "\n",
    " # remove outliers from the time series\n",
    "dfs = drop_veg_outliers(dfs, sigmas=3) # not convinced this is really helping much\n",
    "\n",
    "# plot the feature vectors averaged over all time points and sub images\n",
    "try:\n",
    "    input_dir = os.path.join(output_dir,'preprocessed_data')\n",
    "    plot_feature_vector(input_dir)\n",
    "except AttributeError:\n",
    "    print('Can not plot feature vectors...') \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series analysis\n",
    "\n",
    "Using the data we can build a time series. For this analysis we do the following steps:\n",
    "\n",
    "- Build time series for every sub-image, we drop points with large outliers and smooth the sub-image time series.\n",
    "- We average all the network centrality measures from every sub-image into a single time series.\n",
    "- Compare time series with precipitation data and calculate measures such as correlations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to time series\n",
    "time_series_path = os.path.join(output_dir,'processed_data','time_series.csv')\n",
    "time_series_dfs = pd.read_csv(time_series_path)\n",
    "\n",
    "#corr_subdir = os.path.join(output_dir, \"correlations\")\n",
    "corr_subdir = os.path.join(output_dir, \"correlations\")\n",
    "if not os.path.exists(corr_subdir):\n",

    "    os.makedirs(corr_subdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the cross-correlation between the network centrality measures and precipitation for different lags of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# make cross correlation scatterplot matrix plots\n",
    "plot_cross_correlations(time_series_dfs, corr_subdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# make a smoothed time series plot\n",
    "plot_time_series(time_series_dfs, os.path.join(tsa_subdir,'analysis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore auto-correlation of the time series of all available time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make autocorrelation plots\n",
    "plot_autocorrelation_function(time_series_dfs, corr_subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal and trend analysis\n",
    "\n",
    "The time series shown above show a clear seasonal trend. The STL decomposition implementation from the statsmodels package is applied to the un-smoothed time series to separate the different components. \n",
    "\n",
    "This is done for both the network centrality metrics and precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stl_decomposition(time_series_dfs[['S2_offset50_mean','total_precipitation','date']], 12, os.path.join(output_dir, \"detrended/STL\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

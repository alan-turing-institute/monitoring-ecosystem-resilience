# The `pyveg` Package

## Introduction

The `pyveg` package is developed to study the evolution of vegetation patterns in semi-arid environments using data downloaded from Google Earth Engine.

The code in this repository is intended to perform two main tasks:

**1. Download and process GEE data**:

* Download satellite data from Google Earth Engine (images and weather data).
    * Downloaded images are divided into 50x50 pixel sub-images, network centrality metrics are used to describe the pattern vegetation are then calculated on the sub-image level. Both colour (RGB) and Normalised Difference Vegetation Index (NDVI) images are downloaded and stored on the sub-image level.
    * For weather collections the precipitation and temperature "images" are averaged into a single value at each point in the time series.
* The download job is fully specified by a configuration file that can be generated by specifying the details of the data to be downloaded via prompts (satellite to use, coordinates, time period, number of time points, etc.).


This page contains an installation guide, and some usage examples for this package.


## Installation

`pyveg` requires Python 3.6 or greater. To install, start by creating a fresh `conda` environment.
```
conda create -n veg python=3.7
conda activate veg
```
Get the source.
```
git clone https://github.com/urbangrammarai/gee_pipeline.git
```
Enter the repository and check out a relevant branch if necessary (the default `master` branch contains the most up to date stable version of the code).
```
cd gee_pipeline
```
Install the package using `pip`.
```
pip install .
```
If you are using Windows and encounter issues during this stage, a solution may be found here: https://github.com/NREL/OpenOA/issues/37. If you plan on making changes to the source code, you can instead run `pip install -e .`.

Before using the Google Earth Engine API, you need to sign up with a Google account. You can read more in [here](https://developers.google.com/earth-engine/guides/python_install), how to open an account and authenticate using [gcloud](https://cloud.google.com/sdk/docs/install).
To authenticate for the first time open python and run
```
ee.Initialize()
```
If you are using `gcloud`, it will initialize automatically.



### Google Earth Engine

[Google Earth Engine](https://earthengine.google.com) (GEE) is a powerful tool for obtaining and analysing satellite imagery. This directory contains some useful scripts for interacting with the Earth Engine API. The earth engine API is installed automatically as part of the `pyveg` package installation. If you wish to install it separately, you can follow the instructions [here](https://developers.google.com/earth-engine/python_install_manual).

## Downloading data from GEE with ``pyveg``

### Downloading data from GEE using the CLI

To run a `pyveg` download job, use
```
pyveg_run_pipeline --config_file <path to config>
```

The download job is fully specified by a configuration file, which you point to using the `--config_file` argument. A sample config file with relevant functionality for the urban grammar project is found at `pyveg/configs/config_liverpool_example.py`. You can also optionally specify a string to identify the download job using the `--name` argument.

Note that we use the [BNG convention](https://britishnationalgrid.uk/) for coordinates, i.e. `(eastings,northings)` and we set bounds for regions to download in the convention `(left, bottom, right, top)`.

#### Generating a download configuration file

**TODO: To be updated with peep instructions!**

To create a configuration file for use in the pyveg pipeline described above, use the command
```
pyveg_generate_config
```
this allows the user to specify various characteristics of the data they want to download via prompts. The list in order is as follows:

* `--configs_dir`: The path to the directory containing the config file, with a default option `pyveg/configs`.

* `--collection_name`: The name of the dataset used in the collection, either Sentinel2, or Landsat 8, 7, 5 or 4.
    *    Sentinel2: [Available from 2015-06-23 at 10m resolution.](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2)
    *    Landsat8: [Available from 2013-04-11 at 30m resolution.](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1)
    *    Landsat7: [Available from 1999-01-01 at 30m resolution.](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LE07_C01_T1)
    *    Landsat5: [Available from 1984-03-10 to 2013-01-31 at 60m resolution.](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LT05_C01_T1)
    *    Landsat4: [Available from 1982-07-16 to 1993-12-14 at 60m resolution.](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LT04_C01_T1)

* `--latitude`: The latitude (in degrees north) of the centre point of the image collection.

* `--longitude`: The longitude (in degrees east) of the centre point of the image collection.

* `--country`: The country (for the file name) can either be entered, or use the specified coordinates to look up the country name from the OpenCage database.

* `--start_date`: The start date in the format ‘YYYY-MM-DD’, the default is ‘2015-01-01’ (or ‘2019-01-01’ for a test config file).

* `--end_date`: The end date in the format ‘YYYY-MM-DD’, the default is today’s date (or ‘2019-03-01’ for a test config file).

* `--time_per_point`: The option to run the image collection either monthly (‘1m’) or weekly (‘1w’), with the default being monthly.

* `--run_mode`: The option to run time-consuming functions on Azure (‘batch’) or running locally on your own computer (‘local’). The default is local. For info about running on Azure go [here](UsingAzure.md).

* `--output_dir`: The option to write the output to a specified directory, with the default being the current directory.

* `--test_mode`: The option to make a test config file, containing fewer months and a subset of sub-images, with a default option to have a normal config file.
    *    By choosing the test config file, the start and end dates (see below) are defaulted to cover a smaller time span.
    *    It is recommended that the test config option should be used purely to determine if the options specified by the user are correct.


* `--n_threads`:  Finally, how many threads the user would like to use for the time-consuming processes, either 4 (default) or 8.

For example:
```
 pyveg_generate_config --configs_dir "pyveg/configs" --collection_name "Sentinel2" --latitude 11.58 --longitude 27.94 --start_date "2016-01-01" --end_date "2020-06-30" --time_per_point "1m" --run_mode "local" --n_threads 4
```

This generates a file named `config_Sentinel2_11.58N_27.94E_Sudan_2016-01-01_2020-06-30_1m_local.py` along with instructions on how to use this configuration file to download data through the pipeline, in this case the following:

```
pyveg_run_pipeline --config_file pyveg/configs/config_Sentinel2_11.58N_27.94E_Sudan_2016-01-01_2020-06-30_1m_local.py
```

Individual options can be specified by the user via prompt. The options for this can be found by typing ```pyveg_generate_config --help```.


### More Details on Downloading

During the download job, `pyveg` will break up your specified date range into a time series, and download data at each point in the series. Note that by default the vegetation images downloaded from GEE will be split up into 50x50 pixel images, vegetation metrics are then calculated on the sub-image level. Both colour (RGB) and Normalised Difference Vegetation Index (NDVI) images are downloaded and stored. Vegetation metrics include the mean NDVI pixel intensity across sub-images, and also network centrality metrics, discussed in more detail below.

For weather collections e.g. the ERA5, due to coarser resolution, the precipitation and temperature "images" are averaged into a single value at each point in the time series.

### Rerunning partially succeeded jobs

The output location of a download job is datestamped with the time that the job was launched.  The configuration file used will also be copied and datestamped, to aid reproducibility.  For example if you run the job
```
pyveg_run_pipeline --config_file pyveg/configs/my_config.py
```
there will be a copy of `my_config.py` saved as `pyveg/configs/cached_configs/my_config_<datestamp>.py`. This also means that if a job crashes or timeouts partway through, it is possible to rerun, writing to the same output location and skipping parts that are already done by using this cached config file.  However, in order to avoid a second datestamp being appended to the output location, use the ```--from_cache``` argument.  So for the above example, the command to rerun the job filling in any failed/incomplete parts would be:
```
pyveg_run_pipeline --config_file pyveg/configs/cached_configs/my_config_<datestamp>.py --from_cache
```

### Using Azure for downloading/processing data

If you have access to Microsoft Azure cloud computing facilities, downloading and processing data can be sped up enormously by using batch computing to run many subjobs in parallel.  See [here](UsingAzure.md) for more details.

### Uploading results to the Zenodo open source repository

See [here](UsingZenodo.md) for more details.

# Contributing

We welcome contributions from anyone who is interested in the project. There are lots of ways to contribute, not just writing code. See our [Contributor Guidelines](CONTRIBUTING.md) to learn  more about how you can contribute and how we work together as a community.

# Licence

This project is licensed under the terms of the MIT software license.
